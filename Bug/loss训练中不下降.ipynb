{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyHMu5qlfxsT+kEJN15cw5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leiyi-ihb/ColabPractics/blob/main/Bug/loss%E8%AE%AD%E7%BB%83%E4%B8%AD%E4%B8%8D%E4%B8%8B%E9%99%8D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDDmkZNCFj-m",
        "outputId": "45e0b333-b55f-45e5-c53e-8e7337877c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Platform:Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n",
            "Tensorflow version:2.9.2\n",
            "Keras version:2.9.0\n"
          ]
        }
      ],
      "source": [
        "import platform\n",
        "import tensorflow\n",
        "import keras\n",
        "from numpy.random import seed\n",
        "print(\"Platform:{}\".format(platform.platform()))\n",
        "print(\"Tensorflow version:{}\".format(tensorflow.__version__))\n",
        "print(\"Keras version:{}\".format(keras.__version__))\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Conv1D, MaxPool1D\n",
        "from keras.utils import plot_model\n",
        "from keras.optimizers import Adam, SGD"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! if [ ! -f \"dataset_Li.pkl\" ]; then wget https://master.dl.sourceforge.net/project/summer-research-20/sim_data_1/dataset_Li.pkl ; else echo \"Found previous downloaded data.\"; fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qFlDEF9FmXF",
        "outputId": "80bdd75b-2786-452b-843c-8ddee8851cf4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-12 16:19:10--  https://master.dl.sourceforge.net/project/summer-research-20/sim_data_1/dataset_Li.pkl\n",
            "Resolving master.dl.sourceforge.net (master.dl.sourceforge.net)... 216.105.38.12\n",
            "Connecting to master.dl.sourceforge.net (master.dl.sourceforge.net)|216.105.38.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.sourceforge.net/project/summer-research-20/sim_data_1/dataset_Li.pkl [following]\n",
            "--2022-11-12 16:19:10--  https://downloads.sourceforge.net/project/summer-research-20/sim_data_1/dataset_Li.pkl\n",
            "Resolving downloads.sourceforge.net (downloads.sourceforge.net)... 204.68.111.105\n",
            "Connecting to downloads.sourceforge.net (downloads.sourceforge.net)|204.68.111.105|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cfhcable.dl.sourceforge.net/project/summer-research-20/sim_data_1/dataset_Li.pkl [following]\n",
            "--2022-11-12 16:19:11--  https://cfhcable.dl.sourceforge.net/project/summer-research-20/sim_data_1/dataset_Li.pkl\n",
            "Resolving cfhcable.dl.sourceforge.net (cfhcable.dl.sourceforge.net)... 146.71.73.6\n",
            "Connecting to cfhcable.dl.sourceforge.net (cfhcable.dl.sourceforge.net)|146.71.73.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 640080470 (610M) [application/octet-stream]\n",
            "Saving to: ‘dataset_Li.pkl’\n",
            "\n",
            "dataset_Li.pkl      100%[===================>] 610.43M  42.4MB/s    in 11s     \n",
            "\n",
            "2022-11-12 16:19:22 (57.9 MB/s) - ‘dataset_Li.pkl’ saved [640080470/640080470]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data processing\n",
        "import pickle\n",
        "with open(\"dataset_Li.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "x_train, y_train = np.array(data['train'][0]), np.array(data['train'][1])\n",
        "x_test, y_test = np.array(data['test'][0]), np.array(data['test'][1])\n",
        "\n",
        "print(x_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WfH-TsHFo8M",
        "outputId": "60d35a3b-0a67-4702-9678-5f9d739a2168"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16000, 1000, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.reshaping.flatten import Flatten\n",
        "from keras.layers.regularization.dropout import Dropout\n",
        "class CNNModel: \n",
        "  seed(1)\n",
        "  tensorflow.random.set_seed(1) \n",
        "  def __init__(self):        \n",
        "    '''optimizer'''\n",
        "    # self.adam = Adam(lr=0.01)\n",
        "    # self.sgd = SGD(lr=0.01) \n",
        "    # self.model = Sequential()\n",
        "    pass\n",
        "\n",
        "  def modelBuild(self, filter, poolSize, kernelSize, unit):\n",
        "    # model = self.model\n",
        "    '''model build'''\n",
        "       \n",
        "    input = Input(shape=(1000, 4), name='input')\n",
        "    conv1d_1 = Conv1D(filters=filter, kernel_size=kernelSize, activation='relu', name='conv1d_1')(input)\n",
        "    # pool_1 = MaxPool1D(pool_size=poolSize, name='pool_1')(conv1d_1)\n",
        "    conv1d_2 = Conv1D(filters=filter/2, kernel_size=kernelSize, activation='relu', name='conv1d_2')(conv1d_1)\n",
        "    # pool_2 = MaxPool1D(pool_size=2, name='pool_2')(conv1d_2)\n",
        "    conv1d_3 = Conv1D(filters=filter/4, kernel_size=kernelSize, activation='relu', name='conv1d_3')(conv1d_2)\n",
        "    pool_3 = MaxPool1D(pool_size=2, name='pool_3')(conv1d_3)\n",
        "    dense_1 = Dense(units=unit, activation='relu', name='dense1')(pool_3)\n",
        "    # dense_2 = Dense(8, activation='relu', name='dense2')(dense_1)\n",
        "    # drop = Dropout(0.2, name='dropout_1')(dense_1)\n",
        "    flat_1 = Flatten()(dense_1)\n",
        "    output = Dense(1, activation='softmax', name='output')(flat_1)\n",
        "\n",
        "    model = Model(inputs=input, outputs=output)\n",
        "    return model\n",
        "\n",
        "  def modelCompile(self, opt, m):\n",
        "    '''model compile'''\n",
        "    model = m\n",
        "    model.compile(optimizer=opt,\n",
        "             metrics = 'acc',\n",
        "             loss = 'binary_crossentropy')\n",
        "\n",
        "  def modelFit(self, batSize, m): \n",
        "    '''model fix'''\n",
        "    model = m \n",
        "    # model = self.model \n",
        "    model.fit(x_train, \n",
        "              y_train,\n",
        "              epochs = 10,\n",
        "              batch_size = batSize\n",
        "              )\n",
        "    \n",
        "  def modelEvaluate(self, m):\n",
        "    '''model evaluate'''\n",
        "    model = m\n",
        "    # model = self.model\n",
        "    loss, accuracy1 = model.evaluate(x_train, y_train)\n",
        "    print(\"\\ntest loss:\", loss)\n",
        "    print('accuracy:', accuracy1)\n",
        "    \n",
        "    loss, accuracy2 = model.evaluate(x_test, y_test) #将我们的训练集也评估一下\n",
        "    print(\"\\ntrain loss:\", loss)\n",
        "    print('accuracy:', accuracy2)\n",
        "    \n",
        "    return \"test_loss\" + \"\\t\" + str(accuracy1) + \"\\t\" + \"train_loss\" + \"\\t\" + str(accuracy2)\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "  '''\n",
        "  1, Conv1d: Filters = {32, 16, 3}; Kernel_size = {20, 10, 7, 5, 3}; Activation = {‘relu’, ‘tanh’, ‘linear’}\n",
        "  2, Pooling1d: Max or Avg pool; Pool size = {8, 4, 2,}\n",
        "  3, Flatten: Global Max, Global Avg, Flatten\n",
        "  4, Dense: Units = {32, 64}; Activation = {‘relu’, ‘tanh’, ‘linear’}\n",
        "  '''\n",
        "  filter=[4, 8, 16, 32, 64, 128]  \n",
        "  kernel_size =[20, 10, 7, 5, 53]\n",
        "  poolSize = [8, 4, 2]\n",
        "  unit=[4, 8, 16, 32]\n",
        "  opt=['adam', 'sgd']\n",
        "  batSize=[4, 8, 10, 12]\n",
        "\n",
        "\n",
        "  from itertools import product\n",
        "#   outfile = open(\"outFileBinaryCrossentropy.txt\", \"a+\")\n",
        "  loop_val = [filter, kernel_size, poolSize, unit, opt, batSize] #\n",
        "  for i in product(*loop_val):\n",
        "    # print(i)\n",
        "    chr_tmp = str(i)\n",
        "    if i == (4, 20, 8, 16, 'adam', 8):\n",
        "      outfile = open(\"outFileBC.txt\", \"a+\")\n",
        "      obj1 = CNNModel()\n",
        "      model = obj1.modelBuild(filter=i[0], kernelSize=i[1], poolSize=i[2], unit=i[3])\n",
        "      adam =Adam(lr=0.1)\n",
        "      obj1.modelCompile(opt=adam, m = model)\n",
        "      print(model.summary())\n",
        "      obj1.modelFit(batSize=i[5], m = model)\n",
        "      scor = obj1.modelEvaluate(m=model)\n",
        "    # print(scor)\n",
        "      outfile.write(chr_tmp + \"\\t\" + scor+\"\\n\")\n",
        "      outfile.close()      "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BF_sM7y3z3u",
        "outputId": "e775af2e-2575-4d95-e352-1222c751509b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input (InputLayer)          [(None, 1000, 4)]         0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 981, 4)            324       \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 962, 2)            162       \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 943, 1)            41        \n",
            "                                                                 \n",
            " pool_3 (MaxPooling1D)       (None, 471, 1)            0         \n",
            "                                                                 \n",
            " dense1 (Dense)              (None, 471, 16)           32        \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 7536)              0         \n",
            "                                                                 \n",
            " output (Dense)              (None, 1)                 7537      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,096\n",
            "Trainable params: 8,096\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "2000/2000 [==============================] - 37s 18ms/step - loss: 0.6990 - acc: 0.4997\n",
            "Epoch 2/10\n",
            "2000/2000 [==============================] - 34s 17ms/step - loss: 0.6962 - acc: 0.4997\n",
            "Epoch 3/10\n",
            "2000/2000 [==============================] - 36s 18ms/step - loss: 0.6965 - acc: 0.4997\n",
            "Epoch 4/10\n",
            "2000/2000 [==============================] - 35s 18ms/step - loss: 0.6965 - acc: 0.4997\n",
            "Epoch 5/10\n",
            "2000/2000 [==============================] - 34s 17ms/step - loss: 0.6972 - acc: 0.4997\n",
            "Epoch 6/10\n",
            "2000/2000 [==============================] - 36s 18ms/step - loss: 0.6979 - acc: 0.4997\n",
            "Epoch 7/10\n",
            "2000/2000 [==============================] - 35s 18ms/step - loss: 0.6971 - acc: 0.4997\n",
            "Epoch 8/10\n",
            "2000/2000 [==============================] - 38s 19ms/step - loss: 0.6983 - acc: 0.4997\n",
            "Epoch 9/10\n",
            "2000/2000 [==============================] - 41s 21ms/step - loss: 0.6968 - acc: 0.4997\n",
            "Epoch 10/10\n",
            "2000/2000 [==============================] - 39s 19ms/step - loss: 0.6984 - acc: 0.4997\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6972 - acc: 0.4997\n",
            "\n",
            "test loss: 0.6972287893295288\n",
            "accuracy: 0.4996874928474426\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 0.6984 - acc: 0.5060\n",
            "\n",
            "train loss: 0.6983786225318909\n",
            "accuracy: 0.5059999823570251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**这里总结一下loss函数训练过程中不下降的问题。当然我们这里讨论的loss不下降，默认了数据特征工程，网络结构等是没问题的，仅从参数上讨论不下降的情况，自己踩过的坑，自己填一下。**  \n",
        "我在模型训练过程中发现loss不下降：  \n",
        "1、踩的第一个坑是最后一层输出层，使用的激活函数是'relu'，即同隐藏层的完全一样，`老板说这是有问题的，相当于最后一层没有激活`  \n",
        "2、自己修改后，发现有些情况下还是不下降，所以又查找原因，发现网上有好些这样的问题，有一个的解释是：pytorch的交叉熵（即带有crossentropy的损失函数）里自带softmax，如果在网络输出上手动加softmax就会出现loss不下降[pytorch-分类任务训练loss不变](https://blog.csdn.net/RH_Wang/article/details/110229268)【但是需要注意的这是使用的pytorch，而且针对的是多分类】  \n",
        "**3、整理loss与输出层激活函数的对应关系: [keras里一般sigmoid，损失函数一般对应使用binary_crossentropy；softmax，损失函数一般对应使用categorical_crossentropy](https://blog.csdn.net/weixin_38582851/article/details/82222411)**"
      ],
      "metadata": {
        "id": "HJP_j_QT5gua"
      }
    }
  ]
}