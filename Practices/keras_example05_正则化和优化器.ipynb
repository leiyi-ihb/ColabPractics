{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcPlUpmOQVgx9r6YKAUFoE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leiyi-ihb/ColabPractics/blob/main/Practices/keras_example05_%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "acj3yQaowOMV"
      },
      "outputs": [],
      "source": [
        "# 进阶上节课，加上正则化系数，看看模型是否能好点\n",
        "# 也就是层中加参数kernel_regularizer参数(正则化参数除了这个外，还有其他两个，其实设置哪一个都是可以的)\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        "from keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#输入数据\n",
        "(x_train, y_train), (x_test, y_test)=mnist.load_data()\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_Y8hvV0wcPN",
        "outputId": "d206e840-d823-4b34-cc1a-cf6972b257a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28) (60000,)\n",
            "(10000, 28, 28) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 将(60000, 28, 28)转为(60000, 784)，这里的784其实就是28*28【我的理解应该是降维处理】\n",
        "x_train = x_train.reshape(x_train.shape[0], -1) #行数就是取原来的60000，列数取-1表示不确定，由程序自己判断。 \n",
        "# x_train = x_train.reshape(x_train.shape[0], 784) #跟上面一样的结果\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "print(x_train.shape, len(x_train[0])) #可以发现，数据就变成一个60000行，784列的数据了\n",
        "# print(x_train[0])\n",
        "\n",
        "#需要对数据做个归一化\n",
        "print(x_train.max()) \n",
        "x_train = x_train/255 #知道这里为什么除的是255吗？最大值为255\n",
        "# print(x_train[0])\n",
        "print(x_train.max())\n",
        "\n",
        "x_test = x_test/255\n",
        "\n",
        "# 转换label为one hot形式\n",
        "from keras.utils import np_utils\n",
        "print(y_train[0])\n",
        "y_train = np_utils.to_categorical(y_train, num_classes=10) #这里为啥分为10个类没有讲清楚\n",
        "y_test = np_utils.to_categorical(y_test, num_classes=10) #这里为啥分为10个类没有讲清楚\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPIJQPSF6wOQ",
        "outputId": "57debb7b-1c54-4347-f978-d89cc2704222"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784) 784\n",
            "255\n",
            "1.0\n",
            "5\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建模型\n",
        "model = Sequential([\n",
        "    Dense(units=200, input_dim=784, bias_initializer=\"one\", activation=\"tanh\", kernel_regularizer=l2(0.0003)),\n",
        "    Dense(units=100, bias_initializer=\"one\", activation=\"tanh\", kernel_regularizer=l2(0.0003)),\n",
        "    Dense(units=10, bias_initializer=\"one\", activation=\"softmax\", kernel_regularizer=l2(0.0003))\n",
        "])\n",
        "\n",
        "# 定义优化器，编译模型。一定要注意optimizer别写成Optimizer或optimizers噢噢噢噢\n",
        "sgd = SGD(lr=0.2)\n",
        "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics= ['accuracy']) \n",
        "\n",
        "# 开始训练模型\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# 评估模型\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"\\ntest loss:\", loss)\n",
        "print('accuracy:', accuracy)\n",
        "\n",
        "loss, accuracy = model.evaluate(x_train, y_train) #将我们的训练集也评估一下\n",
        "print(\"\\ntrain loss:\", loss)\n",
        "print('accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjIxcWdy6xIl",
        "outputId": "3f73aeff-96ff-4986-87d3-43c2643f606a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4394 - accuracy: 0.9100\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2631 - accuracy: 0.9603\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2199 - accuracy: 0.9687\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1959 - accuracy: 0.9738\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1807 - accuracy: 0.9751\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1708 - accuracy: 0.9774\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1638 - accuracy: 0.9780\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1585 - accuracy: 0.9793\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1551 - accuracy: 0.9803\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1525 - accuracy: 0.9805\n",
            "313/313 [==============================] - 2s 4ms/step - loss: 0.1733 - accuracy: 0.9722\n",
            "\n",
            "test loss: 0.17326496541500092\n",
            "accuracy: 0.9721999764442444\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1449 - accuracy: 0.9840\n",
            "\n",
            "train loss: 0.1448681354522705\n",
            "accuracy: 0.9839500188827515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**结果：**我们在这节课里加正则化系数的目的就是为了优化上节课的模型，但是从测试集和训练集的准确度，可以看出有一点点改善，但不是很明显。`那么我们什么时候需要加Dropout或正则化系数呢？？`需要根据自己的具体数据来说了，没有一定的标准。"
      ],
      "metadata": {
        "id": "LjU362qhBUkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 接下来看看优化器"
      ],
      "metadata": {
        "id": "vu0TDS4evG_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 直接使用前面的程序\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD, Adam"
      ],
      "metadata": {
        "id": "-XwmnvfPu--9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#输入数据\n",
        "(x_train, y_train), (x_test, y_test)=mnist.load_data()\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)\n",
        "\n",
        "# 将(60000, 28, 28)转为(60000, 784)，这里的784其实就是28*28【我的理解应该是降维处理】\n",
        "x_train = x_train.reshape(x_train.shape[0], -1) #行数就是取原来的60000，列数取-1表示不确定，由程序自己判断。 \n",
        "# x_train = x_train.reshape(x_train.shape[0], 784) #跟上面一样的结果\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "print(x_train.shape, len(x_train[0])) #可以发现，数据就变成一个60000行，784列的数据了\n",
        "# print(x_train[0])\n",
        "\n",
        "#需要对数据做个归一化\n",
        "print(x_train.max()) \n",
        "x_train = x_train/255 #知道这里为什么除的是255吗？最大值为255\n",
        "# print(x_train[0])\n",
        "print(x_train.max())\n",
        "\n",
        "x_test = x_test/255\n",
        "\n",
        "# 转换label为one hot形式\n",
        "from keras.utils import np_utils\n",
        "print(y_train[0])\n",
        "y_train = np_utils.to_categorical(y_train, num_classes=10) #这里为啥分为10个类没有讲清楚\n",
        "y_test = np_utils.to_categorical(y_test, num_classes=10) #这里为啥分为10个类没有讲清楚\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwxHL1xWvD3w",
        "outputId": "2d7fec76-56a8-4dc0-a7f1-71ff2854e7c4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (60000,)\n",
            "(10000, 28, 28) (10000,)\n",
            "(60000, 784) 784\n",
            "255\n",
            "1.0\n",
            "5\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建模型\n",
        "model = Sequential([\n",
        "    Dense(units=10, input_dim=784, bias_initializer=\"one\", activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# 定义优化器，编译模型。一定要注意optimizer别写成Optimizer或optimizers噢噢噢噢\n",
        "# sgd = SGD(lr=0.2)\n",
        "adam = Adam(lr=0.001)\n",
        "# model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics= ['accuracy']) \n",
        "model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics= ['accuracy'])\n",
        "\n",
        "# 开始训练模型\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# 评估模型\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"\\ntest loss:\", loss)\n",
        "print('accuracy:', accuracy)\n",
        "\n",
        "loss, accuracy = model.evaluate(x_train, y_train) #将我们的训练集也评估一下\n",
        "print(\"\\ntrain loss:\", loss)\n",
        "print('accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zUlO-75veQG",
        "outputId": "f39685c5-2f03-428e-e567-dd4cedf9646c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4663 - accuracy: 0.8770\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3036 - accuracy: 0.9143\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2833 - accuracy: 0.9208\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2728 - accuracy: 0.9237\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2666 - accuracy: 0.9250\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2618 - accuracy: 0.9263\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2584 - accuracy: 0.9277\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2550 - accuracy: 0.9293\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2534 - accuracy: 0.9296\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2514 - accuracy: 0.9305\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2635 - accuracy: 0.9269\n",
            "\n",
            "test loss: 0.2634873390197754\n",
            "accuracy: 0.9269000291824341\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2437 - accuracy: 0.9326\n",
            "\n",
            "train loss: 0.24371695518493652\n",
            "accuracy: 0.9325833320617676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**结果：**可以发现，将优化器由sgd改为adam后，结果还差了一点。其实根据经验，大多数时候adam比sgd要好一些，且能够更快的拟合。"
      ],
      "metadata": {
        "id": "EfSJ86hSwbVi"
      }
    }
  ]
}