{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrm9YuI/6qpUrus6M5prvl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leiyi-ihb/ColabPractics/blob/main/B_practices/keras_example04_%E4%BA%A4%E5%8F%89%E7%86%B5%E5%92%8CDropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "acj3yQaowOMV"
      },
      "outputs": [],
      "source": [
        "# 这一节主要针对上一节的MNIST数据集分类进行优化调整\n",
        "# 就只将模型中的loss=\"mse\"改为了loss=\"categorical_crossentropy\"\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#输入数据\n",
        "(x_train, y_train), (x_test, y_test)=mnist.load_data()\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_Y8hvV0wcPN",
        "outputId": "86fd9c47-90e0-4a6e-dcec-faae2adc7b0c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (60000,)\n",
            "(10000, 28, 28) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 将(60000, 28, 28)转为(60000, 784)，这里的784其实就是28*28【我的理解应该是降维处理】\n",
        "x_train = x_train.reshape(x_train.shape[0], -1) #行数就是取原来的60000，列数取-1表示不确定，由程序自己判断。 \n",
        "# x_train = x_train.reshape(x_train.shape[0], 784) #跟上面一样的结果\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "print(x_train.shape, len(x_train[0])) #可以发现，数据就变成一个60000行，784列的数据了\n",
        "# print(x_train[0])\n",
        "\n",
        "#需要对数据做个归一化\n",
        "print(x_train.max()) \n",
        "x_train = x_train/255 #知道这里为什么除的是255吗？最大值为255\n",
        "# print(x_train[0])\n",
        "print(x_train.max())\n",
        "\n",
        "x_test = x_test/255\n",
        "\n",
        "# 转换label为one hot形式\n",
        "from keras.utils import np_utils\n",
        "print(y_train[0])\n",
        "y_train = np_utils.to_categorical(y_train, num_classes=10) #这里为啥分为10个类没有讲清楚\n",
        "y_test = np_utils.to_categorical(y_test, num_classes=10) #这里为啥分为10个类没有讲清楚\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvp_hKQQwfiI",
        "outputId": "d0af820b-9bc0-4d0c-a860-ace84ccb804a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784) 784\n",
            "255\n",
            "1.0\n",
            "5\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建模型\n",
        "model = Sequential([\n",
        "    Dense(units=10, input_dim=784, bias_initializer=\"one\", activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# 定义优化器，编译模型。一定要注意optimizer别写成Optimizer或optimizers噢噢噢噢\n",
        "sgd = SGD(lr=0.2)\n",
        "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics= ['accuracy']) \n",
        "\n",
        "# 开始训练模型\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# 评估模型\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"loss:\", loss)\n",
        "print('accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pxryYcZwh3U",
        "outputId": "0ca3c9db-4d2b-4703-8541-23c0a62defc9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3774 - accuracy: 0.8940\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3025 - accuracy: 0.9149\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2892 - accuracy: 0.9185\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2827 - accuracy: 0.9204\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2780 - accuracy: 0.9223\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2749 - accuracy: 0.9231\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2712 - accuracy: 0.9247\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2690 - accuracy: 0.9252\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2669 - accuracy: 0.9253\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2655 - accuracy: 0.9260\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2755 - accuracy: 0.9247\n",
            "loss: 0.27552467584609985\n",
            "accuracy: 0.9247000217437744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**结果：**可以发现迭代第一次，准确度为0.8940，而上一节中，想要达到接近这个水平，约需要迭代3次，因此这里的效果好一些，但没有讲清楚怎么一下就知道使用这个的原因"
      ],
      "metadata": {
        "id": "LutcDSDJx1Qy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 接下来看看Dropout的使用"
      ],
      "metadata": {
        "id": "zp3qXvDn75W8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.backend import dropout\n",
        "from keras.layers.regularization.dropout import Dropout\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import SGD"
      ],
      "metadata": {
        "id": "JF-ctbTszk8a"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#输入数据\n",
        "(x_train, y_train), (x_test, y_test)=mnist.load_data()\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b2JUueR6Hgk",
        "outputId": "ff936a2a-ae04-4f19-abae-47a49e601d83"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (60000,)\n",
            "(10000, 28, 28) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 将(60000, 28, 28)转为(60000, 784)，这里的784其实就是28*28【我的理解应该是降维处理】\n",
        "x_train = x_train.reshape(x_train.shape[0], -1) #行数就是取原来的60000，列数取-1表示不确定，由程序自己判断。 \n",
        "# x_train = x_train.reshape(x_train.shape[0], 784) #跟上面一样的结果\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "print(x_train.shape, len(x_train[0])) #可以发现，数据就变成一个60000行，784列的数据了\n",
        "# print(x_train[0])\n",
        "\n",
        "#需要对数据做个归一化\n",
        "print(x_train.max()) \n",
        "x_train = x_train/255 #知道这里为什么除的是255吗？最大值为255\n",
        "# print(x_train[0])\n",
        "print(x_train.max())\n",
        "\n",
        "x_test = x_test/255\n",
        "\n",
        "# 转换label为one hot形式\n",
        "from keras.utils import np_utils\n",
        "print(y_train[0])\n",
        "y_train = np_utils.to_categorical(y_train, num_classes=10) #这里为啥分为10个类没有讲清楚\n",
        "y_test = np_utils.to_categorical(y_test, num_classes=10) #这里为啥分为10个类没有讲清楚\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPIJQPSF6wOQ",
        "outputId": "1ca75ad2-f19b-4098-c1c2-5f170c43f9d5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784) 784\n",
            "255\n",
            "1.0\n",
            "5\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建模型\n",
        "model = Sequential([\n",
        "    Dense(units=200, input_dim=784, bias_initializer=\"one\", activation=\"tanh\"),\n",
        "    Dense(units=100, bias_initializer=\"one\", activation=\"tanh\"),\n",
        "    Dense(units=10, bias_initializer=\"one\", activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# 定义优化器，编译模型。一定要注意optimizer别写成Optimizer或optimizers噢噢噢噢\n",
        "sgd = SGD(lr=0.2)\n",
        "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics= ['accuracy']) \n",
        "\n",
        "# 开始训练模型\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# 评估模型\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"\\ntest loss:\", loss)\n",
        "print('accuracy:', accuracy)\n",
        "\n",
        "loss, accuracy = model.evaluate(x_train, y_train) #将我们的训练集也评估一下\n",
        "print(\"\\ntrain loss:\", loss)\n",
        "print('accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjIxcWdy6xIl",
        "outputId": "49807fcf-a8a6-45a9-81da-9aba683986ab"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2812 - accuracy: 0.9148\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1166 - accuracy: 0.9635\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0783 - accuracy: 0.9755\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0584 - accuracy: 0.9817\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0442 - accuracy: 0.9863\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0323 - accuracy: 0.9898\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0243 - accuracy: 0.9927\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0177 - accuracy: 0.9949\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0123 - accuracy: 0.9970\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0080 - accuracy: 0.9985\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0650 - accuracy: 0.9791\n",
            "\n",
            "test loss: 0.06497374922037125\n",
            "accuracy: 0.9790999889373779\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0051 - accuracy: 0.9993\n",
            "\n",
            "train loss: 0.005058887414634228\n",
            "accuracy: 0.9993000030517578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**结果：**可以发现增加网络复杂度之后，准确率有很大的提升。测试集的准确度为0.9790，训练集的准确率为0.9993，很明显测试集同训练集之间存在一定的差距，因此有点过拟合了【其实就是判断两者差异】。其实我们这个例子差距不是很大，但以后很可能就会遇到差距大的，需要注意这一点。"
      ],
      "metadata": {
        "id": "uFzI3Jja9Ysb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建模型\n",
        "model = Sequential([\n",
        "    Dense(units=200, input_dim=784, bias_initializer=\"one\", activation=\"tanh\"),\n",
        "    Dropout(0.4), #其实就是让上面的神经元的40%不工作\n",
        "    Dense(units=100, bias_initializer=\"one\", activation=\"tanh\"),\n",
        "    Dropout(0.4),\n",
        "    Dense(units=10, bias_initializer=\"one\", activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# 定义优化器，编译模型。一定要注意optimizer别写成Optimizer或optimizers噢噢噢噢\n",
        "sgd = SGD(lr=0.2)\n",
        "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics= ['accuracy']) \n",
        "\n",
        "# 开始训练模型\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# 评估模型\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"\\ntest loss:\", loss)\n",
        "print('accuracy:', accuracy)\n",
        "\n",
        "loss, accuracy = model.evaluate(x_train, y_train) #将我们的训练集也评估一下\n",
        "print(\"\\ntrain loss:\", loss)\n",
        "print('accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHtpotAZAXl3",
        "outputId": "e454b868-2074-4430-8ae1-3952f5a246e8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.4485 - accuracy: 0.8640\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2877 - accuracy: 0.9157\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2423 - accuracy: 0.9288\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2161 - accuracy: 0.9364\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1981 - accuracy: 0.9409\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1880 - accuracy: 0.9441\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1760 - accuracy: 0.9478\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1701 - accuracy: 0.9497\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1594 - accuracy: 0.9513\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1522 - accuracy: 0.9544\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0999 - accuracy: 0.9706\n",
            "\n",
            "test loss: 0.09986058622598648\n",
            "accuracy: 0.9706000089645386\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0693 - accuracy: 0.9788\n",
            "\n",
            "train loss: 0.06933464109897614\n",
            "accuracy: 0.9788333177566528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**结果：**在这个例子中可以发现，测试集最后的准确度还不如上次高了【说明次例子并不适合dropout，只有凑合这讲】，但Droput使得测试和训练准确度间的差异确实缩小了一些，但是还是还是存在些许过拟合（所以这个并非是讲解Dropout的最好例子）  \n",
        "还有一个特点可以发现，训练过程中出现的准确度(最高为0.9544)都是比最终出来的准确度(0.9706)小的，这是因为训练过程使用Dropout,因此并没有使用全神经元做判断，而最后的准确度计算是包含全神经元的。"
      ],
      "metadata": {
        "id": "LjU362qhBUkz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wlXHoa4w9XR5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}